# @package _global_

# usage: +experiment=simclr

defaults:
    - override /dm: whaledo
    - override /backbone: beit
    - override /alg: simclr
    - override /logger: whaledo
    - _self_

backbone:
  pretrained: true

dm: 
  test_prop: 0.1
  image_size: 224
  training_mode: step
  train_batch_size: 32
  train_transforms: 
    # _target_: torchvision.transforms.Compose
    # transforms: 
    # - _target_: whaledo.transforms.ResizeAndPadToSize
    #   size: ${ dm.image_size }
    # - _target_: whaledo.transforms.MultiCropTransform.with_dino_transform
    #   global_crop_size: ${ dm.image_size }
    #   local_crops_number: 0
    _target_: torchvision.transforms.Compose
    transforms: 
    # - _target_: whaledo.transforms.ResizeAndPadToSize
    #   size: ${ dm.image_size }
    # - _target_: whaledo.transforms.MultiCropTransform.with_dino_transform
    #   global_crop_size: ${ dm.image_size }
    #   local_crops_number: 0
    - _target_: whaledo.transforms.MultiCropTransform.with_whaledo_transform
      global_crop_size: ${ dm.image_size }

trainer:
  max_steps: 25000
  multiple_trainloader_mode: 'min_size'
  val_check_interval: 500
  precision: 16
  accumulate_grad_batches: null

alg:
  base_lr: 1e-4
  temp0: 0.1
  weight_decay: 1.e-1
  learn_temp: false
  proj_dim: 256
  mlp_head: true
  scheduler_cls: whaledo.schedulers.CosineLRWithLinearWarmup
  scheduler_kwargs:
    warmup_iters: 0.1
    total_iters: ${ trainer.max_steps }
    lr_start: 5e-7

logger:
  group: simclr

