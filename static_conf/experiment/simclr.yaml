# @package _global_

# usage: +experiment=simclr

defaults:
    - override /dm: whaledo
    - override /backbone: beit
    - override /alg: simclr
    - override /logger: whaledo
    - override /checkpointer: whaledo
    - _self_

backbone:
  pretrained: true

dm: 
  test_prop: 0.1
  image_size: 224
  training_mode: step
  train_batch_size: 32
  train_transforms: 
    _target_: torchvision.transforms.Compose
    transforms: 
    # - _target_: whaledo.transforms.ResizeAndPadToSize
    #   size: ${ dm.image_size }
    # - _target_: whaledo.transforms.MultiCropTransform.with_dino_transform
    #   global_crop_size: ${ dm.image_size }
    #   local_crops_number: 0
    - _target_: whaledo.transforms.MultiCropTransform.with_whaledo_transform
      global_crop_size: ${ dm.image_size }

trainer:
  max_steps: 10000
  multiple_trainloader_mode: 'min_size'
  val_check_interval: 250
  precision: 16
  accumulate_grad_batches: null
  sync_batchnorm: true

alg:
  base_lr: 1e-4
  temp_start: 0.1
  temp_end: 0.1
  temp_warmup_steps: ${ trainer.max_steps }
  weight_decay: 1.e-1
  proj_depth: 2
  out_dim: 256
  mlp_dim: 4096
  scheduler_cls: whaledo.schedulers.CosineLRWithLinearWarmup
  scheduler_kwargs:
    warmup_iters: 0.1
    total_iters: ${ trainer.max_steps }
    lr_start: 5e-7

logger:
  group: simclr

